version: "3.8"

services:
  # Redis service for caching and session management
  redis:
    image: redis:7-alpine
    container_name: hashmancer-redis-gpu
    restart: unless-stopped
    ports:
      - "127.0.0.1:6379:6379"
    volumes:
      - redis-data:/data
      - ./config/redis.conf:/usr/local/etc/redis/redis.conf:ro
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - hashmancer-gpu-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp

  # GPU-enabled Hashmancer server
  hashmancer-gpu:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    container_name: hashmancer-server-gpu
    restart: unless-stopped
    ports:
      - "${HTTP_PORT:-8000}:8000"
    depends_on:
      redis:
        condition: service_healthy
    environment:
      # Core configuration
      - REDIS_URL=redis://redis:6379
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      
      # Admin configuration
      - ADMIN_USERNAME=${ADMIN_USERNAME:-admin}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD}
      
      # API keys (optional)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      
      # GPU configuration
      - CUDA_VISIBLE_DEVICES=all
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_CACHE_PATH=/app/gpu_cache
      - NUMBA_CACHE_DIR=/app/gpu_cache/numba
      - TORCH_CUDA_ARCH_LIST=6.0;6.1;7.0;7.5;8.0;8.6+PTX
      - FORCE_CUDA=1
      
      # Security settings
      - SECRET_KEY=${SECRET_KEY:-}
      - ALLOWED_HOSTS=${ALLOWED_HOSTS:-localhost,127.0.0.1}
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost:8000}
      
      # Performance settings (optimized for GPU)
      - MAX_WORKERS=${MAX_WORKERS:-2}
      - WORKER_TIMEOUT=${WORKER_TIMEOUT:-600}
      - MAX_UPLOAD_SIZE=${MAX_UPLOAD_SIZE:-1GB}
      - GPU_MEMORY_FRACTION=${GPU_MEMORY_FRACTION:-0.8}
      
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FORMAT=${LOG_FORMAT:-json}
    
    volumes:
      - hashmancer-data:/app/data
      - hashmancer-logs:/app/logs
      - hashmancer-config:/app/config
      - hashmancer-temp:/app/temp
      - hashmancer-uploads:/app/uploads
      - hashmancer-gpu-cache:/app/gpu_cache
      
    networks:
      - hashmancer-gpu-network
      
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -f http://localhost:8000/health && python -c 'import torch; assert torch.cuda.is_available()'"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
      
    security_opt:
      - no-new-privileges:true
      
    # GPU resource allocation
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          memory: 2G
          cpus: '1.0'
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # GPU-enabled worker service
  worker-gpu:
    build:
      context: .
      dockerfile: Dockerfile.worker
    container_name: hashmancer-worker-gpu
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
      hashmancer-gpu:
        condition: service_healthy
    environment:
      - REDIS_URL=redis://redis:6379
      - SERVER_URL=http://hashmancer-gpu:8000
      - WORKER_ID=${WORKER_ID:-gpu-worker-1}
      - WORKER_THREADS=${WORKER_THREADS:-8}
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      
      # GPU configuration for worker
      - CUDA_VISIBLE_DEVICES=all
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - USE_GPU=true
      - GPU_DEVICE_ID=${GPU_DEVICE_ID:-0}
    
    volumes:
      - hashmancer-worker-data:/app/data
      - hashmancer-wordlists:/app/wordlists
      - hashmancer-rules:/app/rules
      - hashmancer-gpu-cache:/app/gpu_cache
      
    networks:
      - hashmancer-gpu-network
      
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
      
    security_opt:
      - no-new-privileges:true
      
    # GPU resource allocation for worker
    deploy:
      resources:
        limits:
          memory: 12G
          cpus: '8.0'
        reservations:
          memory: 4G
          cpus: '2.0'
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # GPU monitoring service
  nvidia-smi-exporter:
    image: mindprince/nvidia_gpu_prometheus_exporter:0.1
    container_name: hashmancer-gpu-monitor
    restart: unless-stopped
    ports:
      - "127.0.0.1:9445:9445"
    networks:
      - hashmancer-gpu-network
    profiles:
      - monitoring
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Nginx reverse proxy (optional)
  nginx:
    image: nginx:alpine
    container_name: hashmancer-nginx-gpu
    restart: unless-stopped
    ports:
      - "${HTTPS_PORT:-443}:443"
      - "80:80"
    depends_on:
      - hashmancer-gpu
    volumes:
      - ./config/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./config/ssl:/etc/nginx/ssl:ro
      - nginx-cache:/var/cache/nginx
      - nginx-logs:/var/log/nginx
    networks:
      - hashmancer-gpu-network
    profiles:
      - nginx
    security_opt:
      - no-new-privileges:true

  # Monitoring with Prometheus (optional)
  prometheus:
    image: prom/prometheus:latest
    container_name: hashmancer-prometheus-gpu
    restart: unless-stopped
    ports:
      - "127.0.0.1:9090:9090"
    volumes:
      - ./config/prometheus-gpu.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    networks:
      - hashmancer-gpu-network
    profiles:
      - monitoring
    security_opt:
      - no-new-privileges:true

  # Grafana for metrics visualization (optional)
  grafana:
    image: grafana/grafana:latest
    container_name: hashmancer-grafana-gpu
    restart: unless-stopped
    ports:
      - "127.0.0.1:3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana:/etc/grafana/provisioning:ro
    networks:
      - hashmancer-gpu-network
    profiles:
      - monitoring
    security_opt:
      - no-new-privileges:true

volumes:
  # Core application volumes
  redis-data:
    driver: local
  hashmancer-data:
    driver: local
  hashmancer-logs:
    driver: local
  hashmancer-config:
    driver: local
  hashmancer-temp:
    driver: local
  hashmancer-uploads:
    driver: local
  hashmancer-gpu-cache:
    driver: local
  
  # Worker volumes
  hashmancer-worker-data:
    driver: local
  hashmancer-wordlists:
    driver: local
  hashmancer-rules:
    driver: local
  
  # Nginx volumes
  nginx-cache:
    driver: local
  nginx-logs:
    driver: local
  
  # Monitoring volumes
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

networks:
  hashmancer-gpu-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16